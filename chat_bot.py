import pandas as pd
import streamlit as st
import requests
from streamlit_searchbox import st_searchbox

# === App Title ===
st.title("ğŸ“ Call Center Chatbot")

# === User Guide ===
with st.expander("â„¹ï¸ HÆ°á»›ng dáº«n sá»­ dá»¥ng chatbot", expanded=False):
    st.info("""
    **ğŸ“˜ Call Center Chatbot - HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng**

    **1. Nháº­p tá»« khÃ³a**  
    ğŸ” GÃµ tá»« khÃ³a vÃ o Ã´ tÃ¬m kiáº¿m (vÃ­ dá»¥: *há»c phÃ­, há»c bá»•ng, Ä‘Äƒng kÃ½, lá»‹ch há»c*...).  
    Chatbot sáº½ tá»± Ä‘á»™ng gá»£i Ã½ nhá»¯ng tá»« phÃ¹ há»£p.

    **2. Hoáº·c chá»n tá»« khÃ³a á»Ÿ thanh bÃªn**  
    ğŸ“‚ Chá»n má»™t chá»§ Ä‘á» vÃ  tá»« khÃ³a trong thanh bÃªn trÃ¡i Ä‘á»ƒ xem cÃ¢u tráº£ lá»i.

    **3. Dá»¯ liá»‡u tá»± Ä‘á»™ng cáº­p nháº­t**  
    ğŸ“‚ Dá»¯ liá»‡u Ä‘Æ°á»£c láº¥y tá»« GitHub vÃ  lÃ m sáº¡ch trÆ°á»›c khi hiá»ƒn thá»‹.  
    Há»‡ thá»‘ng chá»‰ giá»¯ láº¡i phiÃªn báº£n má»›i nháº¥t cá»§a má»—i tá»« khÃ³a.

    **ğŸ›  GÃ³p Ã½ & BÃ¡o lá»—i**  
    Vui lÃ²ng liÃªn há»‡ nhÃ³m phÃ¡t triá»ƒn táº¡i: [GitHub Repo](https://github.com/Menbeo/-HUHU-)
    """)

# === GitHub Repo Info ===
GITHUB_USER = "mintus2511"
GITHUB_REPO = "CC_Chatbot"
GITHUB_API_URL = f"https://api.github.com/repos/{GITHUB_USER}/{GITHUB_REPO}/contents/"

# === Step 1: Get CSV files from GitHub ===
@st.cache_data(ttl=60)
def get_csv_file_links():
    try:
        response = requests.get(GITHUB_API_URL)
        response.raise_for_status()
        files = response.json()

        sorted_csvs = sorted(
            [file for file in files if file["name"].endswith(".csv")],
            key=lambda x: x["name"]
        )

        return {
            file["name"]: file["download_url"]
            for file in sorted_csvs
        }
    except Exception as e:
        st.error(f"âŒ Lá»—i khi káº¿t ná»‘i tá»›i GitHub: {e}")
        return {}

# === Step 2: Load & clean CSVs ===
@st.cache_data(ttl=60)
def load_csvs(csv_files):
    combined = pd.DataFrame(columns=["key word", "description", "topic"])

    for name, url in csv_files.items():
        try:
            df = pd.read_csv(url)
            df.columns = df.columns.str.lower().str.strip()
            if {"key word", "description"}.issubset(df.columns):
                df["topic"] = name.replace(".csv", "")
                combined = pd.concat([combined, df[["key word", "description", "topic"]]], ignore_index=True)
        except Exception as e:
            st.warning(f"âš ï¸ Lá»—i Ä‘á»c {name}: {e}")

    # âœ… Keep only the latest version of each "key word"
    combined = combined.drop_duplicates(subset="key word", keep="last")

    # âœ… Remove duplicate descriptions
    dupes = combined[combined.duplicated("description", keep=False)].copy()
    removed_duplicates = dupes[dupes.duplicated("description", keep="first")]
    cleaned_data = combined.drop_duplicates(subset="description", keep="first")

    return cleaned_data, removed_duplicates

# === Step 3: Load data ===
csv_files = get_csv_file_links()
data, removed_duplicates = load_csvs(csv_files)

# === Step 4: UI Logic ===
if not data.empty:
    all_keywords = sorted(data["key word"].dropna().astype(str).unique())
    all_topics = sorted(data["topic"].dropna().unique())

    # === SIDEBAR: Topic & Keyword Picker ===
    with st.sidebar:
        with st.expander("ğŸ¯ Chá»n nhanh theo chá»§ Ä‘á» & tá»« khÃ³a", expanded=True):
            selected_topic = st.selectbox("ğŸ“ Chá»§ Ä‘á»", [""] + all_topics)

            if selected_topic:
                topic_data = data[data["topic"] == selected_topic]
                topic_keywords = sorted(topic_data["key word"].dropna().astype(str).unique())
                selected_kw = st.selectbox("ğŸ”‘ Tá»« khÃ³a", [""] + topic_keywords)

                if selected_kw:
                    st.session_state["selected_keyword"] = selected_kw

        with st.expander("ğŸ“š Danh má»¥c táº¥t cáº£ tá»« khÃ³a", expanded=False):
            st.markdown("DÆ°á»›i Ä‘Ã¢y lÃ  toÃ n bá»™ danh sÃ¡ch tá»« khÃ³a theo tá»«ng chá»§ Ä‘á»:")

            for topic in all_topics:
                st.markdown(f"#### ğŸ“ {topic}")
                topic_data = data[data["topic"] == topic]
                keywords = sorted(topic_data["key word"].dropna().astype(str).unique())
                for kw in keywords:
                    st.markdown(f"- ğŸ”‘ `{kw}`")

    # === MAIN SEARCH BOX ===
    def search_fn(user_input):
        return [kw for kw in all_keywords if user_input.lower() in kw.lower()]

    selected_keyword = st_searchbox(
        search_fn,
        key="keyword_search",
        label="ğŸ” GÃµ tá»« khÃ³a",
        placeholder="VÃ­ dá»¥: há»c phÃ­, há»c bá»•ng, Ä‘Äƒng kÃ½..."
    )

    if selected_keyword:
        st.session_state["selected_keyword"] = selected_keyword

    # === RESULT: Show chatbot response ===
    if "selected_keyword" in st.session_state:
        keyword = st.session_state["selected_keyword"]
        matches = data[data["key word"].str.lower().str.contains(keyword.lower(), na=False)]

        if not matches.empty:
            st.subheader(f"Káº¿t quáº£ cho tá»« khÃ³a: `{keyword}`")
            for _, row in matches.iterrows():
                st.write("ğŸ¤– **Bot:**", row["description"])
        else:
            st.info("KhÃ´ng tÃ¬m tháº¥y mÃ´ táº£ cho tá»« khÃ³a nÃ y.")
else:
    st.error("âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u há»£p lá»‡.")
